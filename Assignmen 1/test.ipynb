{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/guesejustin/object-recognition-mlp-cnn-efficientnet-on-cifar\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.utils import to_categorical # FIX ask if this is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import np_utils\n",
    "import keras\n",
    "# one-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# break training set into training and validation sets\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# print shape of training set\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_valid.shape[0], 'validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first construct a basic mlp model, feel free to play around!\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = x_train.shape[1:]))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "import time\n",
    "# train our model and save the best results in the file: MLP.best_weights\n",
    "# additionally count the time how long it took\n",
    "\n",
    "mlp_start = time.time() # for stopwatch\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=32, epochs=20,\n",
    "          validation_data=(x_valid, y_valid), verbose=2, shuffle=True)\n",
    "\n",
    "mlp_end = time.time()\n",
    "mlp_took = mlp_end -mlp_start\n",
    "print(\"took %s seconds\"%(mlp_took))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - Split the new train and validation (test) set - 80/20 ratio\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(kernel_initializer,learning_rate,dropout_rate):\n",
    "    '''\n",
    "    Parameters for optimal:\n",
    "    > kernel_initializer = 'glorot_uniform'(default),'glorot_normal','he_uniform','he_normal'           \n",
    "    > learning_rate = 0.001, 0.01(default), 0.1, ...\n",
    "    > dropout_rate = 0.1, 0.2(default), 0.3, ...\n",
    "    '''\n",
    "    # Build the model\n",
    "    mlp_model = Sequential([\n",
    "        # Flatten the Input\n",
    "        keras.layers.Flatten(input_shape=(32,32,3),name='Flatten'),\n",
    "        # First Hidden Layer（less neurons） - ReLU activation\n",
    "        keras.layers.Dense(512, activation='relu',name='Hidden_1st'),\n",
    "        # Second Hidden Layer (less neurons) - ReLU activation\n",
    "        keras.layers.Dense(256, activation='relu',name='Hidden_2nd'),\n",
    "        # Output Layer - Softmax activation\n",
    "        keras.layers.Dense(10, activation='softmax',name='Output_Softmax') \n",
    "    ],name='MLP_model')\n",
    "    \n",
    "    # Compile the model (learning_rate used here)\n",
    "    mlp_model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 - Callbacks the Learning rate scheduling\n",
    "\n",
    "# Learning rate - Exponential scheduler\n",
    "def Exponential_scheduler(epoch,lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.1*epoch)\n",
    "    \n",
    "# Callbacks learning rate scheduling and early stopping\n",
    "lrs = LearningRateScheduler(Exponential_scheduler)\n",
    "es = EarlyStopping('val_loss',patience=3,restore_best_weights=True)\n",
    "callbacks = [es,lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MLP_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " Hidden_1st (Dense)          (None, 512)               1573376   \n",
      "                                                                 \n",
      " Hidden_2nd (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " Output_Softmax (Dense)      (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1707274 (6.51 MB)\n",
      "Trainable params: 1707274 (6.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = build_mlp_model(kernel_initializer='he_normal',learning_rate=0.1,dropout_rate=0.2)\n",
    "mlp_model.summary() # Display its architucture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 23:10:36.847237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-08-25 23:10:45.541222: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 - 11s - loss: nan - accuracy: 0.1003 - val_loss: nan - val_accuracy: 0.0986 - lr: 0.0100 - 11s/epoch - 8ms/step\n",
      "Epoch 2/100\n",
      "1250/1250 - 8s - loss: nan - accuracy: 0.1003 - val_loss: nan - val_accuracy: 0.0986 - lr: 0.0100 - 8s/epoch - 6ms/step\n",
      "Epoch 3/100\n",
      "1250/1250 - 7s - loss: nan - accuracy: 0.1003 - val_loss: nan - val_accuracy: 0.0986 - lr: 0.0100 - 7s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "HundredEpochs_history = mlp_model.fit(X_train,y_train,epochs=100,callbacks=callbacks, validation_data=(X_valid,y_valid),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
